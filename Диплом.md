Диплом 



преновначальная настройка ОС

hostname && hostname -i
hostname && hostname -i
hostname && hostname -i


masterkubernetes.ru-central1.internal
10.128.0.20

kb1.ru-central1.internal
10.128.0.10

kb2.ru-central1.internal
10.129.0.28

kb3.ru-central1.internal
10.129.0.11



Шаг 1: Предварительные требования 
1.a.. Проверьте ОС, конфигурацию оборудования и Сетевое подключение 
1.b.. Отключите подкачку Disable SWAP и брандмауэр 

sudo yum -y install epel-release
yum install -y htop mc vim wget telnet

sudo sed -i '/swap/d' /etc/fstab && sudo swapoff -a 
sudo systemctl stop firewalld 
sudo systemctl disable firewalld 

1.c Отключить Selinux 
Контейнеры должны получить доступ к файловой системе хоста. SELinux должен быть
установлен в разрешающий режим, который эффективно отключает его функции безопасности.


sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
sudo setenforce 0


Шаг 2. Настройте локальные таблицы IP для просмотра мостового трафика 
2.a.. Включите мостовой трафик 

sudo modprobe br_netfilter && lsmod | grep br_netfilter

2.b.. Скопируйте приведенное ниже содержимое в этот файл.. /etc/modules-load.d/k8s.conf
cat <<EOF | > sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF


Скопируйте приведенное ниже содержимое в этот файл.. /etc/sysctl.d/k8s.conf 
cat <<EOF | > sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF


sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF


# Ensure you load modules
sudo modprobe overlay && sudo modprobe br_netfilter && sudo sysctl --system



Шаг 3. Установите Docker будем работать через его движок

3.a.. Удалите все старые версии если чтото было установлено
sudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine

3. б.. Установите утилиты Yum | Диспетчер конфигурации  
sudo yum install -y yum-utils

3.c.. Настройте репозиторий Docker
yum-config-manager  --add-repo https://download.docker.com/linux/centos/docker-ce.repo

cat <<EOF | > sudo tee /etc/yum.repos.d/docker.repo
[docker]
baseurl=https://download.docker.com/linux/centos/7/x86_64/stable/
gpgcheck=0
EOF

3. d.. Установите Docker Engine, Docker CLI, Docker RUNTIME $ 
yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin


4.b. Скопируйте приведенное ниже содержимое в этот файл.. /etc/docker/
sudo tee /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF


sudo systemctl enable docker && sudo systemctl restart docker && sudo systemctl status docker
sudo systemctl daemon-reload


Шаг 5. Установите kubeadm, kubectl, kubelet 
5.a.. Скопируйте в этот файл приведенное ниже содержимое.. /etc/yum.repos.d/kubernetes.repo
cat <<EOF | > sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

смотрим какие версии доступны для установки
yum --showduplicates list kubeadm.x86_64
yum --showduplicates list kubeadm.x86_64
yum --showduplicates list kubeadm.x86_64


Устанавливаем командой new
yum install -y kubelet-1.21.0-0.x86_64 kubeadm-1.21.0-0.x86_64 kubectl-1.21.0-0.x86_64
yum install -y kubelet-1.21.0-0.x86_64 kubeadm-1.21.0-0.x86_64 kubectl-1.21.0-0.x86_64
yum install -y kubelet-1.21.0-0.x86_64 kubeadm-1.21.0-0.x86_64 kubectl-1.21.0-0.x86_64

Устанавливаем командой old
yum install -y kubelet-1.16.2-0.x86_64 kubeadm-1.16.2-0.x86_64 kubectl-1.16.2-0.x86_64


Я использовал следующую команду для создания конфигурации по умолчанию (при новой установке на master):
kubeadm init --pod-network-cidr=10.10.0.0/16 --apiserver-advertise-address=master_nodeIP
kubeadm init --pod-network-cidr="10.10.0.0/16" --apiserver-advertise-address="10.128.0.20"
kubeadm init --pod-network-cidr="10.10.0.0/16" --apiserver-advertise-address="10.128.0.20"
или
kubeadm init phase kubelet-start
kubeadm init phase kubelet-start

где:
--pod-network-cidr=10.10.0.0/16 - диапазон сети pod
--apiserver-advertise-address=master_nodeIP  - ip адрес кластера(главного узла кластера) вставляем сюда ip VM на которой инициализируем кластер

кластер готов
----------------------------------------------------------------------------------------------------------------------------
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.128.0.20:6443 --token mlmf5i.6la6cbrxy9via93q \
        --discovery-token-ca-cert-hash sha256:e596f8d4c4d599c68b1bb0beee8bc2885aa87c792f3f83e1d0e91dedbeac38fc
		
----------------------------------------------------------------------------------------------------------------------------


версия
kubectl version
kubectl version
kubectl version


Управляйте кластером как обычный пользователь
Чтобы начать использовать кластер, вам нужно запустить его как обычный пользователь, набрав:
Чтобы начать использовать свой кластер, вам необходимо запустить следующее как обычный пользователь: 
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config



Настройте сеть Pod
Pod Network позволяет узлам внутри кластера взаимодействовать. 
Существует несколько доступных сетевых вариантов Kubernetes. Используйте следующую команду для установки сетевой надстройки flannel pod:
https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model

https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#install
https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#install


# Запуск только в главном узле 
# Запускать только в мастер-ноде настроим сеть для подов
для Weave Networks:
с помошью этой команды будет установлен плагин Weave со всеми необходимыми разрешениями для кластера
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml


Шаг 9. Присоедините рабочие узлы к мастеру
Присоедините рабочий узел к кластеру
Как указано в шаге 1 , вы можете использовать kubeadm join команду на каждом рабочем узле, чтобы подключить его к кластеру.
Замените коды на коды с вашего главного сервера. Повторите это действие для каждого рабочего узла в вашем кластере.
# Запуск в рабочих узлах как "Root" 

kubeadm join 10.128.0.20:6443 --token mlmf5i.6la6cbrxy9via93q \
        --discovery-token-ca-cert-hash sha256:e596f8d4c4d599c68b1bb0beee8bc2885aa87c792f3f83e1d0e91dedbeac38fc

где:
10.128.0.20:6443 -- адрес главного узла куба мастер ноды !!!!

после запуска команды на присоединение мы мониторим когда рабочий узел кластера будет готов
после запуска команды на присоединение мы мониторим когда рабочий узел кластера будет готов
после запуска команды на присоединение мы мониторим когда рабочий узел кластера будет готов
чтобы получить информацию о количестве узлов в кластере(мы должны увидеть что в кластере доступен только главный узел)!!!!!
kubectl get nodes
kubectl get nodes
kubectl get nodes


vorori@masterkubernetes ~]$ kubectl get nodes
NAME                                    STATUS   ROLES                  AGE     VERSION
kb1.ru-central1.internal                Ready    <none>                 61s     v1.21.0
kb2.ru-central1.internal                Ready    <none>                 56s     v1.21.0
kb3.ru-central1.internal                Ready    <none>                 49s     v1.21.0
masterkubernetes.ru-central1.internal   Ready    control-plane,master   7m55s   v1.21.0


выполнил на kвсех присоединенных нодах
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config     ### копирую клюс с мастера vim $HOME/.kube/config    ### cat $HOME/.kube/config 
sudo chown $(id -u):$(id -g) $HOME/.kube/config


проверяю на каждой ноде
[vorori@kb3 ~]$ kubectl get nodes
NAME                                    STATUS   ROLES                  AGE     VERSION
kb1.ru-central1.internal                Ready    <none>                 7m52s   v1.21.0
kb2.ru-central1.internal                Ready    <none>                 7m47s   v1.21.0
kb3.ru-central1.internal                Ready    <none>                 7m40s   v1.21.0
masterkubernetes.ru-central1.internal   Ready    control-plane,master   14m     v1.21.0



------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------


https://habr.com/ru/companies/flant/articles/328756/
https://habr.com/ru/companies/flant/articles/328756/


Развернуть CockroachDB в GKE или GCE
Потесировать dataset с чикагскими такси
Или залить 10Гб данных и протестировать скорость запросов в сравнении с 1 инстансом PostgreSQL
Описать что и как делали и с какими проблемами столкнулись


wget https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/cockroachdb-statefulset.yaml

vim cockroachdb-statefulset.yaml
storage: 15Gi

kubectl create -f cockroachdb-statefulset.yaml

