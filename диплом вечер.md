0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------
ИГРА!!!!!!!!!!!!!!!!!
0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------
-------------------------------------для тестов из вне------------------------------------------------
------------------------------------------------------------------------------------------------------

#проверяю что все ок и что все запустилось все под создались и работают
kubectl get pods --namespace spilo
kubectl get pods --namespace spilo

#проверяю что создались сервисы
kubectl get services --namespace spilo
kubectl get services --namespace spilo

#проверяю что все ок и что все запустилось с дисками pvc
kubectl get pvc --namespace spilo
kubectl get pvc --namespace spilo

#проверяю что все ок на всех обектах кластера
kubectl get all -A
kubectl get all -A

#подключиться к поду внутрь контенера с patroni
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- bash
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- bash

#смотрим на кворум кто мастер 
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl list
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl list

#единоразово смотрим логи pod видм кто у нас матер
kubectl logs --namespace spilo pod/zalandopatroni777-0
kubectl logs --namespace spilo pod/zalandopatroni777-1

#смотреть логи в режиме реального времени pod pod/zalandopatroni01-0 namespace spilo используя префикс -f
kubectl logs --namespace spilo pod/zalandopatroni777-0 -f
kubectl logs --namespace spilo pod/zalandopatroni777-0 -f

#Наборы скриптов для patroni находятся тут
kubectl -n spilo exec -it pod/zalandopatroni777-0 -- ls /scripts/
kubectl -n spilo exec -it pod/zalandopatroni777-0 -- ls /scripts/

kubectl -n spilo exec -it pod/zalandopatroni777-0 -- cat /scripts/basebackup.sh
kubectl -n spilo exec -it pod/zalandopatroni777-0 -- cat /scripts/basebackup.sh

#запустить скрипт бекапа
kubectl -n spilo exec -it pod/zalandopatroni777-0 -- bash /scripts/basebackup.sh
kubectl -n spilo exec -it pod/zalandopatroni777-0 -- bash /scripts/basebackup.sh

#если требуется restart конфигурацию кластера 
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl restart zalandopatroni777
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl restart zalandopatroni777

#если требуется reload конфигурацию кластера 
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl reload zalandopatroni777
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl reload zalandopatroni777

#выполняю switchover
kubectl -n spilo exec -it pod/zalandopatroni777-1  -- patronictl switchover
kubectl -n spilo exec -it pod/zalandopatroni777-1  -- patronictl switchover


------------------------------------------------------------------------------------------------------
-------------------------------------для тестов внутри контенера--------------------------------------
------------------------------------------------------------------------------------------------------
# для patroni
sv stop cron
sv restart patroni

#показать лидера и кворум 
patronictl -c postgres.yml list
patronictl -c postgres.yml list

#команда на изменение config
patronictl -c postgres.yml edit-config
patronictl -c postgres.yml edit-config

#если требуется restart
patronictl -c postgres.yml restart zalandopatroni777
patronictl -c postgres.yml restart zalandopatroni777

#если требуется reload
patronictl -c postgres.yml reload zalandopatroni777
patronictl -c postgres.yml reload zalandopatroni777

#преключить wal
select pg_switch_wal();
select pg_switch_wal();

#БАЗОВАЯ ИНФОРМАЦИЯ порт версия сервер
select
inet_server_addr( ) AS "Server",
inet_server_port( ) AS "Port",
current_database() AS "CurrentDatabase",
version() AS "Version";

#показать ноды реплики
select usename,application_name,client_addr,backend_start,state,sync_state from pg_stat_replication;
select usename,application_name,client_addr,backend_start,state,sync_state from pg_stat_replication;

#показать состояние конкретной ноды к которой подключился не находится ли она в состоянии восстоновления
select pg_is_in_recovery();
select pg_is_in_recovery();

============================================================================================
============================================================================================
============================================================================================
# пытаюсь починить archive_command

https://github.com/zalando/postgres-operator/issues/1233
https://github.com/zalando/postgres-operator/issues/1233
============================================================================================

КОМАНДА КОТОРАЯ СРАБОТАЛА!!!!!!!!!! ВРУЧНУЮ создать бекап wal файла 000000020000000000000007!!!!!!!!!!!!!
wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"

/usr/local/bin/wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
/usr/local/bin/wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
/usr/local/bin/wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"

envdir /config /usr/local/bin/wal-g wal-push /home/postgres/pgdata/pgroot/data/pg_wal/000000010000000000000009
============================================================================================
============================================================================================
============================================================================================






============================================================================================
============================================================================================
============================================================================================
# восстановление кластера из бекапа wal-g с журналами на точку времени
https://github.com/zalando/patroni/issues/1571
https://github.com/zalando/patroni/issues/1571
============================================================================================


# моя заметка 
# WAL-G 4 команды. Это:
WAL-PUSH – заархивировать вал.

WAL-FETCH – получить вал.

BACKUP-PUSH – сделать бэкап.

BACKUP-FETCH – получить бэкап из системы резервного копирования.


----------------------------------------------------------------------------------------------------------
Спасибо за помощь! Теперь он работает, так что вот мой файл конфигурации и скрипты. 
Это рабочая комбинация для Postgresql 12 + Patroni + WAL-G (поддерживаемый преемник WAL-E) + Azure/AW
----------------------------------------------------------------------------------------------------------

#как восстанавливаться все расписсанно по щагам
----------------------------------------------------------------------------------------------------------
Предупреждение: ваш каталог /data должен быть доступен для записи пользователю postgres.

Шаги к PITR:

1)
Остановите Patroni на всех узлах реплик и, наконец, на мастере
sudo systemctl stop Patroni

2)
Обновить файл конфигурации /etc/patroni.yml
recovery_target_time: '2020-06-08 08:52:00'

3)
Удалить кластер из etcd
patronictl -c /etc/patroni.yml remove postgres

4)
Резервное копирование и удаление каталога данных на мастере /data/patroni

5)
Запустите Patroni на мастере — он автоматически вызовет скрипт clone_with_walg.sh
sudo systemctl start patchi
----------------------------------------------------------------------------------------------------------


#пример который работает с восстановлением
----------------------------------------------------------------------------------------------------------
scope: postgres
namespace: /db/
name: postgresql1

restapi:
    listen: 10.0.1.4:8008
    connect_address: 10.0.1.4:8008

etcd:
    host: 10.0.1.7:2379

bootstrap:
    dcs:
        ttl: 30
        loop_wait: 10
        retry_timeout: 10
        maximum_lag_on_failover: 1048576
        postgresql:
            use_pg_rewind: true

    method: clone_with_walg
    clone_with_walg:
        command: /home/postgres/clone_with_walg.sh
        recovery_conf:
            restore_command: envdir /etc/wal-g.d/env wal-g wal-fetch "%f" "%p"
            recovery_target_timeline: latest
            recovery_target_action: promote
            recovery_target_time: ''

    initdb:
    - encoding: UTF8
    - data-checksums

    pg_hba:
    - host replication replicator 127.0.0.1/32 md5
    - host replication replicator 10.0.1.4/0 md5
    - host replication replicator 10.0.1.8/0 md5
    - host replication replicator 10.0.1.6/0 md5
    - host all all 0.0.0.0/0 md5

postgresql:
    listen: 10.0.1.4:5432
    connect_address: 10.0.1.4:5432
    data_dir: /data/patroni
    pgpass: /tmp/pgpass
    authentication:
        replication:
            username: replicator
            password: rep-pass
        superuser:
            username: postgres
            password: secretpassword
    parameters:
        unix_socket_directories: '/var/run/postgresql'
        shared_preload_libraries: 'pg_stat_statements'
        archive_mode: 'on'
        archive_timeout: 300s
        archive_command: 'envdir /etc/wal-g.d/env wal-g wal-push %p'
    recovery_conf:
        restore_command: 'envdir /etc/wal-g.d/env wal-g wal-fetch "%f" "%p"'

tags:
    nofailover: false
    noloadbalance: false
    clonefrom: false
    nosync: false
----------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------
И простой скрипт clone_with_walg.sh:

#!/bin/bash

mkdir -p /data/patroni
envdir /etc/wal-g.d/env wal-g backup-fetch /data/patroni LATEST
----------------------------------------------------------------------------------------------------------





----------------------------------------------------------------------------------------------------------
#ЧЕРНОВИК
https://github.com/zalando/postgres-operator/blob/master/docs/administrator.md
https://github.com/zalando/patroni/issues/1571
https://blog.csdn.net/chenhongloves/article/details/123236332
https://blog.csdn.net/chenhongloves/article/details/123236332
https://blog.csdn.net/chenhongloves/article/details/123236332

В зависимости от поставщика облачного хранилища для Spilo должны быть установлены разные переменные среды . 
Не все они генерируются автоматически оператором путем изменения его конфигурации. 
В этом случае вы должны использовать дополнительную карту конфигурации или секрет .

#выполним backup
envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data
envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data
envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data

В Postgres вы можете проверить предварительно настроенные команды для архивации и восстановления файлов WAL. 
Вы можете найти файлы журналов для соответствующих команд в разделе $HOME/pgdata/pgroot/pg_log/postgres-?.log.

archive_mode: 'on'
archive_command:  `envdir "{WALE_ENV_DIR}" {WALE_BINARY} wal-push "%p"`
restore_command:  `envdir "{{WALE_ENV_DIR}}" /scripts/restore_command.sh "%f" "%p"`


Вы можете создать базовую резервную копию вручную с помощью следующей команды и проверить, попадает ли она в указанный вами путь резервного копирования WAL:
envdir "/run/etc/wal-e.d/env" /scripts/postgres_backup.sh "/home/postgres/pgdata/pgroot/data"

Вы также можете проверить, может ли Spilo найти какие-либо резервные копии:
envdir "/run/etc/wal-e.d/env" wal-g backup-list
----------------------------------------------------------------------------------------------------------

============================================================================================
============================================================================================
============================================================================================




============================================================================================
============================================================================================
============================================================================================
#создаем окружения для  скрипта backup
--------------------------
rm -rf /data/spilo_backup-script_suka.yaml
vim /data/spilo_backup-script_suka.yaml
cat /data/spilo_backup-script_suka.yaml
--------------------------

#применяю
--------------------------
kubectl apply -f /data/spilo_backup-script_suka.yaml
kubectl apply -f /data/spilo_backup-script_suka.yaml
--------------------------

#проверяем
--------------------------
kubectl get ConfigMap
kubectl get ConfigMap
--------------------------

#если надо удалить
--------------------------
kubectl delete configmap backup-script
kubectl delete configmap backup-script
--------------------------

--------------------------
--------------------------
--------------------------
--------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-script
data:
  PGHOST: "/var/run/postgresql"
  PGUSER: "postgres"
  PGROOT: "/home/postgres/pgdata/pgroot"
  PGLOG: "/home/postgres/pgdata/pgroot/pg_log"
  PGDATA: "/home/postgres/pgdata/pgroot/data"
  BACKUP_NUM_TO_RETAIN: "5"
  USE_WALG_BACKUP: "true"
  USE_WALG_RESTORE: "true"
  WAL_BUCKET_SCOPE_PREFIX: ""
  WAL_BUCKET_SCOPE_SUFFIX: ""
  WALG_ALIVE_CHECK_INTERVAL: "5m"
  WALE_BINARY: "wal-g"
  WALG_FILE_PREFIX: "/data/pg_wal"
  CLONE_USE_WALG_RESTORE: "true"
  WALG_DISABLE_S3_SSE: "true"
  WALE_ENV_DIR: "/config
--------------------------
--------------------------
--------------------------
--------------------------


============================================================================================
============================================================================================
============================================================================================


============================================================================================
============================================================================================
============================================================================================
#начинаем создание нашего кластера

#создаем окружения для  основной конфигурации
-------------------------- 
rm -rf /data/spilo_kubernetes_final_suka.yaml
vim /data/spilo_kubernetes_final_suka.yaml
cat /data/spilo_kubernetes_final_suka.yaml
--------------------------


#создаем namespase spilo
--------------------------
kubectl create ns spilo
kubectl create ns spilo
--------------------------

#применяю
--------------------------
kubectl apply -f /data/spilo_kubernetes_final_suka.yaml --namespace spilo
kubectl apply -f /data/spilo_kubernetes_final_suka.yaml --namespace spilo
--------------------------

#проверяю
--------------------------
# pods
kubectl get pods --namespace spilo
kubectl get pods --namespace spilo

# pwc
kubectl get pvc --namespace spilo
kubectl get pvc --namespace spilo

# Это предоставит информацию о пространствах имен
kubectl get namespace
kubectl get namespace
--------------------------


#если надо удалить
--------------------------
#если надо удалить манифест
kubectl delete -f /data/spilo_kubernetes_final_suka.yaml  --namespace spilo
kubectl delete -f /data/spilo_kubernetes_final_suka.yaml  --namespace spilo

#если надо удалить  all,ing,secrets,pvc,pv
kubectl delete all,ing,secrets,pvc,pv --all --namespace spilo
kubectl delete all,ing,secrets,pvc,pv --all --namespace spilo

#после удаляю руками persistentvolume которые не удалились автоматом  из директории  
rm -rf /data/local-path-provisioner/*
rm -rf /data/local-path-provisioner/*
rm -rf /data/local-path-provisioner/*

# дополнительно если надо удалить  namespace spilo
kubectl delete namespace spilo
kubectl delete namespace spilo
--------------------------


--------------------------
--------------------------
--------------------------
--------------------------
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
spec:
  selector:
    matchLabels:
      application: spilo
      spilo-cluster: *cluster_name
  replicas: 3
  serviceName: *cluster_name
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        application: spilo
        spilo-cluster: *cluster_name
    spec:
      # service account that allows changing endpoints and assigning pod labels
      # in the given namespace: https://kubernetes.io/docs/user-guide/service-accounts/
      # not required unless you've changed the default service account in the namespace
      # used to deploy Spilo
      serviceAccountName: operator
      containers:
      - name: *cluster_name
        image: registry.opensource.zalan.do/acid/spilo-15:3.0-p1  # put the spilo image here
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8008
          protocol: TCP
        - containerPort: 5432
          protocol: TCP
        volumeMounts:
        - mountPath: /data/pg_wal
          name: backup
        - mountPath: /config
          name: config
        - mountPath: /home/postgres/pgdata
          name: pgdata
        env:
        - name: DCS_ENABLE_KUBERNETES_API
          value: 'true'
#        - name: ETCD_HOST
#          value: 'test-etcd.default.svc.cluster.local:2379' # where is your etcd?
#        - name: WAL_S3_BUCKET
#          value: example-spilo-dbaas
#        - name: LOG_S3_BUCKET # may be the same as WAL_S3_BUCKET
#          value: example-spilo-dbaas
#        - name: BACKUP_SCHEDULE
#          value: "00 01 * * *"
        - name: KUBERNETES_SCOPE_LABEL
          value: spilo-cluster
        - name: KUBERNETES_ROLE_LABEL
          value: role
        - name: SPILO_CONFIGURATION
          value: | ## https://github.com/zalando/patroni#yaml-configuration
            bootstrap:
              dcs:
                ttl: 100
                loop_wait: 10
                retry_timeout: 10
                maximum_lag_on_failover: 1048576
                master_start_timeout: 300
                synchronous_mode_strict: false
                postgresql:
                  use_pg_rewind: true
                  use_slots: true
                  parameters:
                    max_connections: 101
                    shared_buffers : 2GB
                    effective_cache_size : 7GB
                    maintenance_work_mem: 512MB
                    wal_buffers: 16MB
                    wal_keep_size: 2GB
                    work_mem: 32MB
                    min_wal_size: 1GB
                    max_wal_size: 4GB
                    huge_pages: off
                    max_worker_processes: 4
                    max_parallel_workers: 4
                    max_parallel_workers_per_gather: 2
                    max_parallel_maintenance_workers: 2
                    autovacuum: on
                    autovacuum_max_workers: 4
                    autovacuum_vacuum_scale_factor: 0.01
                    autovacuum_analyze_scale_factor: 0.03
                    autovacuum_vacuum_cost_limit: 500
                    autovacuum_vacuum_cost_delay: 2
                    autovacuum_naptime: 15s
                    autovacuum_vacuum_threshold: 20
                    wal_writer_delay : 200ms
                    wal_writer_flush_after : 1MB
                    random_page_cost: 1.1
                    seq_page_cost: 1
                    effective_io_concurrency: 200
                    superuser_reserved_connections: 4
                    max_locks_per_transaction: 64
                    max_prepared_transactions: 0
                    checkpoint_timeout: 10min
                    checkpoint_completion_target: 0.9
                    default_statistics_target: 1000
                    synchronous_commit: on
                    max_files_per_process: 1024
                    wal_level: replica
                    max_wal_senders: 10
                    max_replication_slots: 10
                    hot_standby: on
                    wal_compression: on
                    track_io_timing: on
                    log_lock_waits: on
                    log_temp_files: 0
                    track_activities: on
                    track_counts: on
                    track_functions: all
                    log_checkpoints: off
                    log_connections: off
                    log_disconnections: off
                    log_statement: none
                    logging_collector: on
                    log_min_duration_statement: 30s
                    log_truncate_on_rotation: on
                    log_rotation_age: 1d
                    log_rotation_size: 0
                    log_line_prefix: '%m [%p] %d %u %h (transaction ID %x)'
                    max_standby_streaming_delay: 30s
                    wal_receiver_status_interval: 10s
                    jit: off
                    lc_messages: en_US.UTF-8
                    track_commit_timestamp: "off"
                    wal_log_hints: on
                    hot_standby_feedback: off
                  pg_hba:
                  - hostnossl all     vororinew      all          md5
              initdb:
                - encoding: UTF8
                - data-checksums
              pg_hba:
              - hostnossl all         vorori         all          md5
            postgresql:
                parameters:
                  log_destination: 'stderr'
                  log_line_prefix: '%m [%p] %d %u %h (transaction ID %x)'
                  archive_mode: on
                  archive_command: 'envdir /config /usr/local/bin/wal-g wal-push "%p"'
                recovery_conf:
                  restore_command: 'envdir /config /usr/local/bin/wal-g wal-fetch "%f" "%p"'
                pg_hba:
                - local     all         all                    trust
                - hostssl   all         +zalandos 127.0.0.1/32 pam
                - host      all         all       127.0.0.1/32 md5
                - hostssl   all         +zalandos ::1/128      pam
                - host      all         all       ::1/128      md5
                - local     replication standby                trust
                - hostssl   replication standby   all          md5
                - hostnossl all         all       all          md5
                - hostssl   all         +zalandos all          pam
                - hostssl   all         all       all          md5
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: PGPASSWORD_SUPERUSER
          valueFrom:
            secretKeyRef:
              name: *cluster_name
              key: superuser-password
        - name: PGUSER_ADMIN
          value: superadmin
        - name: PGPASSWORD_ADMIN
          valueFrom:
            secretKeyRef:
              name: *cluster_name
              key: admin-password
        - name: PGPASSWORD_STANDBY
          valueFrom:
            secretKeyRef:
              name: *cluster_name
              key: replication-password
        - name: SCOPE
          value: *cluster_name
        - name: PGROOT
          value: /home/postgres/pgdata/pgroot
        - name: WALG_FILE_PREFIX
          value: "/home/postgres/pgdata/pgroot/pg_log"
        - name: CRONTAB
          value: "[\"00 01 * * * envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data\"]"
      terminationGracePeriodSeconds: 0
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: db
                    operator: In
                    values:
                      - spilo
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: spilo-cluster
                    operator: In
                    values:
                      - *cluster_name
              topologyKey: "kubernetes.io/hostname"
      volumes:
        - configMap:
            name: backup-script
          name: config
        - persistentVolumeClaim:
            claimName: zalandopatroni777-backup
          name: backup
  volumeClaimTemplates:
  - metadata:
      labels:
        application: spilo
        spilo-cluster: *cluster_name
      name: pgdata
    spec:
      storageClassName: local-path
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
  - metadata:
      labels:
        application: spilo
        spilo-cluster: *cluster_name
      name: backup
    spec:
      storageClassName: local-path
      accessModes:
       - ReadWriteOnce
      resources:
        requests:
          storage: 3Gi
---
apiVersion: v1
kind: Endpoints
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
subsets: []

---
apiVersion: v1
kind: Service
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
spec:
  type: ClusterIP
  ports:
  - name: postgresql
    port: 5432
    targetPort: 5432

---
# headless service to avoid deletion of patronidemo-config endpoint
apiVersion: v1
kind: Service
metadata:
  name: zalandopatroni777-config
  labels:
    application: spilo
    spilo-cluster: zalandopatroni777
spec:
  clusterIP: None

---
apiVersion: v1
kind: Secret
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
type: Opaque
stringData:
  superuser-password: pass1
  replication-password: pass2
  admin-password: pass3

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: operator

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: operator
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
  # delete is required only for 'patronictl remove'
  - delete
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - get
  - patch
  - update
  # the following three privileges are necessary only when using endpoints
  - create
  - list
  - watch
  # delete is required only for for 'patronictl remove'
  - delete
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - patch
  - update
  - watch
# The following privilege is only necessary for creation of headless service
# for patronidemo-config endpoint, in order to prevent cleaning it up by the
# k8s master. You can avoid giving this privilege by explicitly creating the
# service like it is done in this manifest (lines 160..169)
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - create

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: operator
subjects:
- kind: ServiceAccount
  name: operator
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-script
data:
  PGHOST: "/var/run/postgresql"
  PGUSER: "postgres"
  PGROOT: "/home/postgres/pgdata/pgroot"
  PGLOG: "/home/postgres/pgdata/pgroot/pg_log"
  PGDATA: "/home/postgres/pgdata/pgroot/data"
  BACKUP_NUM_TO_RETAIN: "5"
  USE_WALG_BACKUP: "true"
  USE_WALG_RESTORE: "true"
  WAL_BUCKET_SCOPE_PREFIX: ""
  WAL_BUCKET_SCOPE_SUFFIX: ""
  WALG_ALIVE_CHECK_INTERVAL: "5m"
  WALE_BINARY: "wal-g"
  WALG_FILE_PREFIX: "/data/pg_wal"
  CLONE_USE_WALG_RESTORE: "true"
  WALG_DISABLE_S3_SSE: "true"
  WALE_ENV_DIR: "/config"
--------------------------
--------------------------
--------------------------
--------------------------



0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------







