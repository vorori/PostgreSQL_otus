0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------
ИГРА!!!!!!!!!!!!!!!!!
0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------
#для тестов из вне:

kubectl get pods --namespace spilo
kubectl get pods --namespace spilo
kubectl get pods --namespace spilo


kubectl -n spilo exec -it pod/zalandopatroni777-0  -- bash
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- bash
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- bash



#для внутрянки:
patronictl -c postgres.yml list
patronictl -c postgres.yml list
patronictl -c postgres.yml list


patronictl -c postgres.yml edit-config
patronictl -c postgres.yml edit-config
patronictl -c postgres.yml edit-config


select pg_switch_wal();
select pg_switch_wal();
select pg_switch_wal();



============================================================================================
============================================================================================
============================================================================================
# пытаюсь починить archive_command

https://github.com/zalando/postgres-operator/issues/1233
https://github.com/zalando/postgres-operator/issues/1233
============================================================================================

КОМАНДА КОТОРАЯ СРАБОТАЛА!!!!!!!!!! ВРУЧНУЮ создать бекап wal файла 000000020000000000000007!!!!!!!!!!!!!
wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"

/usr/local/bin/wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
/usr/local/bin/wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
/usr/local/bin/wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"

envdir /config /usr/local/bin/wal-g wal-push /home/postgres/pgdata/pgroot/data/pg_wal/000000010000000000000009
============================================================================================
============================================================================================
============================================================================================






============================================================================================
============================================================================================
============================================================================================
# восстановление кластера из бекапа wal-g с журналами на точку времени
https://github.com/zalando/patroni/issues/1571
https://github.com/zalando/patroni/issues/1571
============================================================================================


# моя заметка 
# WAL-G 4 команды. Это:
WAL-PUSH – заархивировать вал.

WAL-FETCH – получить вал.

BACKUP-PUSH – сделать бэкап.

BACKUP-FETCH – получить бэкап из системы резервного копирования.


----------------------------------------------------------------------------------------------------------
Спасибо за помощь! Теперь он работает, так что вот мой файл конфигурации и скрипты. 
Это рабочая комбинация для Postgresql 12 + Patroni + WAL-G (поддерживаемый преемник WAL-E) + Azure/AW
----------------------------------------------------------------------------------------------------------

#как восстанавливаться все расписсанно по щагам
----------------------------------------------------------------------------------------------------------
Предупреждение: ваш каталог /data должен быть доступен для записи пользователю postgres.

Шаги к PITR:

1)
Остановите Patroni на всех узлах реплик и, наконец, на мастере
sudo systemctl stop Patroni

2)
Обновить файл конфигурации /etc/patroni.yml
recovery_target_time: '2020-06-08 08:52:00'

3)
Удалить кластер из etcd
patronictl -c /etc/patroni.yml remove postgres

4)
Резервное копирование и удаление каталога данных на мастере /data/patroni

5)
Запустите Patroni на мастере — он автоматически вызовет скрипт clone_with_walg.sh
sudo systemctl start patchi
----------------------------------------------------------------------------------------------------------


#пример который работает с восстановлением
----------------------------------------------------------------------------------------------------------
scope: postgres
namespace: /db/
name: postgresql1

restapi:
    listen: 10.0.1.4:8008
    connect_address: 10.0.1.4:8008

etcd:
    host: 10.0.1.7:2379

bootstrap:
    dcs:
        ttl: 30
        loop_wait: 10
        retry_timeout: 10
        maximum_lag_on_failover: 1048576
        postgresql:
            use_pg_rewind: true

    method: clone_with_walg
    clone_with_walg:
        command: /home/postgres/clone_with_walg.sh
        recovery_conf:
            restore_command: envdir /etc/wal-g.d/env wal-g wal-fetch "%f" "%p"
            recovery_target_timeline: latest
            recovery_target_action: promote
            recovery_target_time: ''

    initdb:
    - encoding: UTF8
    - data-checksums

    pg_hba:
    - host replication replicator 127.0.0.1/32 md5
    - host replication replicator 10.0.1.4/0 md5
    - host replication replicator 10.0.1.8/0 md5
    - host replication replicator 10.0.1.6/0 md5
    - host all all 0.0.0.0/0 md5

postgresql:
    listen: 10.0.1.4:5432
    connect_address: 10.0.1.4:5432
    data_dir: /data/patroni
    pgpass: /tmp/pgpass
    authentication:
        replication:
            username: replicator
            password: rep-pass
        superuser:
            username: postgres
            password: secretpassword
    parameters:
        unix_socket_directories: '/var/run/postgresql'
        shared_preload_libraries: 'pg_stat_statements'
        archive_mode: 'on'
        archive_timeout: 300s
        archive_command: 'envdir /etc/wal-g.d/env wal-g wal-push %p'
    recovery_conf:
        restore_command: 'envdir /etc/wal-g.d/env wal-g wal-fetch "%f" "%p"'

tags:
    nofailover: false
    noloadbalance: false
    clonefrom: false
    nosync: false
----------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------
И простой скрипт clone_with_walg.sh:

#!/bin/bash

mkdir -p /data/patroni
envdir /etc/wal-g.d/env wal-g backup-fetch /data/patroni LATEST
----------------------------------------------------------------------------------------------------------


============================================================================================
============================================================================================
============================================================================================




============================================================================================
============================================================================================
============================================================================================
#создаем окружения для  скрипта backup
rm -rf /data/spilo_backup-script_suka.yaml
vim /data/spilo_backup-script_suka.yaml
cat /data/spilo_backup-script_suka.yaml

#применяю
kubectl apply -f /data/spilo_backup-script_suka.yaml
kubectl apply -f /data/spilo_backup-script_suka.yaml

#проверяем
kubectl get ConfigMap
kubectl get ConfigMap
kubectl get ConfigMap

#если надо удалить
kubectl delete configmap backup-script
kubectl delete configmap backup-script

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-script
data:
  PGHOST: "/var/run/postgresql"
  PGUSER: "postgres"
  PGROOT: "/home/postgres/pgdata/pgroot"
  PGLOG: "/home/postgres/pgdata/pgroot/pg_log"
  PGDATA: "/home/postgres/pgdata/pgroot/data"
  BACKUP_NUM_TO_RETAIN: "5"
  USE_WALG_BACKUP: "true"
  USE_WALG_RESTORE: "true"
  WAL_BUCKET_SCOPE_PREFIX: ""
  WAL_BUCKET_SCOPE_SUFFIX: ""
  WALG_ALIVE_CHECK_INTERVAL: "5m"
  WALE_BINARY: "wal-g"
  WALG_FILE_PREFIX: "/data/pg_wal"
  CLONE_USE_WALG_RESTORE: "true"
  WALG_DISABLE_S3_SSE: "true"
  WALE_ENV_DIR: "/config



============================================================================================
============================================================================================
============================================================================================


============================================================================================
============================================================================================
============================================================================================

#создаем окружения для  основной конфигурации 
rm -rf /data/spilo_kubernetes_final_suka.yaml
vim /data/spilo_kubernetes_final_suka.yaml
cat /data/spilo_kubernetes_final_suka.yaml

#применяю
kubectl apply -f /data/spilo_kubernetes_final_suka.yaml --namespace spilo
kubectl apply -f /data/spilo_kubernetes_final_suka.yaml --namespace spilo


kubectl delete -f /data/spilo_kubernetes_final_suka.yaml  --namespace spilo
kubectl delete -f /data/spilo_kubernetes_final_suka.yaml  --namespace spilo

kubectl delete all,ing,secrets,pvc,pv --all --namespace spilo
kubectl delete all,ing,secrets,pvc,pv --all --namespace spilo

после удаляю руками persistentvolume которые не удалились автоматом  из директории  
rm -rf /data/local-path-provisioner/*
rm -rf /data/local-path-provisioner/*
rm -rf /data/local-path-provisioner/*


kubectl get pods --namespace spilo
kubectl get pods --namespace spilo
kubectl get pods --namespace spilo


apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
spec:
  selector:
    matchLabels:
      application: spilo
      spilo-cluster: *cluster_name
  replicas: 3
  serviceName: *cluster_name
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        application: spilo
        spilo-cluster: *cluster_name
    spec:
      # service account that allows changing endpoints and assigning pod labels
      # in the given namespace: https://kubernetes.io/docs/user-guide/service-accounts/
      # not required unless you've changed the default service account in the namespace
      # used to deploy Spilo
      serviceAccountName: operator
      containers:
      - name: *cluster_name
        image: registry.opensource.zalan.do/acid/spilo-15:3.0-p1  # put the spilo image here
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8008
          protocol: TCP
        - containerPort: 5432
          protocol: TCP
        volumeMounts:
        - mountPath: /data/pg_wal
          name: backup
        - mountPath: /config
          name: config
        - mountPath: /home/postgres/pgdata
          name: pgdata
        env:
        - name: DCS_ENABLE_KUBERNETES_API
          value: 'true'
#        - name: ETCD_HOST
#          value: 'test-etcd.default.svc.cluster.local:2379' # where is your etcd?
#        - name: WAL_S3_BUCKET
#          value: example-spilo-dbaas
#        - name: LOG_S3_BUCKET # may be the same as WAL_S3_BUCKET
#          value: example-spilo-dbaas
#        - name: BACKUP_SCHEDULE
#          value: "00 01 * * *"
        - name: KUBERNETES_SCOPE_LABEL
          value: spilo-cluster
        - name: KUBERNETES_ROLE_LABEL
          value: role
        - name: SPILO_CONFIGURATION
          value: | ## https://github.com/zalando/patroni#yaml-configuration
            bootstrap:
              dcs:
                ttl: 100
                loop_wait: 10
                retry_timeout: 10
                maximum_lag_on_failover: 1048576
                master_start_timeout: 300
                synchronous_mode_strict: false
                postgresql:
                  use_pg_rewind: true
                  use_slots: true
                  parameters:
                    max_connections: 101
                    shared_buffers : 2GB
                    effective_cache_size : 7GB
                    maintenance_work_mem: 512MB
                    wal_buffers: 16MB
                    wal_keep_size: 2GB
                    work_mem: 32MB
                    min_wal_size: 1GB
                    max_wal_size: 4GB
                    huge_pages: off
                    max_worker_processes: 4
                    max_parallel_workers: 4
                    max_parallel_workers_per_gather: 2
                    max_parallel_maintenance_workers: 2
                    autovacuum: on
                    autovacuum_max_workers: 4
                    autovacuum_vacuum_scale_factor: 0.01
                    autovacuum_analyze_scale_factor: 0.03
                    autovacuum_vacuum_cost_limit: 500
                    autovacuum_vacuum_cost_delay: 2
                    autovacuum_naptime: 15s
                    autovacuum_vacuum_threshold: 20
                    wal_writer_delay : 200ms
                    wal_writer_flush_after : 1MB
                    random_page_cost: 1.1
                    seq_page_cost: 1
                    effective_io_concurrency: 200
                    superuser_reserved_connections: 4
                    max_locks_per_transaction: 64
                    max_prepared_transactions: 0
                    checkpoint_timeout: 10min
                    checkpoint_completion_target: 0.9
                    default_statistics_target: 1000
                    synchronous_commit: on
                    max_files_per_process: 1024
                    wal_level: replica
                    max_wal_senders: 10
                    max_replication_slots: 10
                    hot_standby: on
                    wal_compression: on
                    track_io_timing: on
                    log_lock_waits: on
                    log_temp_files: 0
                    track_activities: on
                    track_counts: on
                    track_functions: all
                    log_checkpoints: off
                    log_connections: off
                    log_disconnections: off
                    log_statement: none
                    logging_collector: on
                    log_min_duration_statement: 30s
                    log_truncate_on_rotation: on
                    log_rotation_age: 1d
                    log_rotation_size: 0
                    log_line_prefix: '%m [%p] %d %u %h (transaction ID %x)'
                    max_standby_streaming_delay: 30s
                    wal_receiver_status_interval: 10s
                    jit: off
                    lc_messages: en_US.UTF-8
                    track_commit_timestamp: "off"
                    wal_log_hints: on
                    hot_standby_feedback: off
                  pg_hba:
                  - hostnossl all     vororinew      all          md5
              initdb:
                - encoding: UTF8
                - data-checksums
              pg_hba:
              - hostnossl all         vorori         all          md5
            postgresql:
                parameters:
                  log_destination: 'stderr'
                  log_line_prefix: '%m [%p] %d %u %h (transaction ID %x)'
                  archive_mode: on
                  archive_command: 'envdir /config /usr/local/bin/wal-g wal-push "%p"'
                recovery_conf:
                  restore_command: 'envdir /config /usr/local/bin/wal-g wal-fetch "%f" "%p"'
                pg_hba:
                - local     all         all                    trust
                - hostssl   all         +zalandos 127.0.0.1/32 pam
                - host      all         all       127.0.0.1/32 md5
                - hostssl   all         +zalandos ::1/128      pam
                - host      all         all       ::1/128      md5
                - local     replication standby                trust
                - hostssl   replication standby   all          md5
                - hostnossl all         all       all          md5
                - hostssl   all         +zalandos all          pam
                - hostssl   all         all       all          md5
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: PGPASSWORD_SUPERUSER
          valueFrom:
            secretKeyRef:
              name: *cluster_name
              key: superuser-password
        - name: PGUSER_ADMIN
          value: superadmin
        - name: PGPASSWORD_ADMIN
          valueFrom:
            secretKeyRef:
              name: *cluster_name
              key: admin-password
        - name: PGPASSWORD_STANDBY
          valueFrom:
            secretKeyRef:
              name: *cluster_name
              key: replication-password
        - name: SCOPE
          value: *cluster_name
        - name: PGROOT
          value: /home/postgres/pgdata/pgroot
        - name: WALG_FILE_PREFIX
          value: "/home/postgres/pgdata/pgroot/pg_log"
        - name: CRONTAB
          value: "[\"00 01 * * * envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data\"]"
      terminationGracePeriodSeconds: 0
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: db
                    operator: In
                    values:
                      - spilo
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: spilo-cluster
                    operator: In
                    values:
                      - *cluster_name
              topologyKey: "kubernetes.io/hostname"
      volumes:
        - configMap:
            name: backup-script
          name: config
        - persistentVolumeClaim:
            claimName: zalandopatroni777-backup
          name: backup
  volumeClaimTemplates:
  - metadata:
      labels:
        application: spilo
        spilo-cluster: *cluster_name
      name: pgdata
    spec:
      storageClassName: local-path
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
  - metadata:
      labels:
        application: spilo
        spilo-cluster: *cluster_name
      name: backup
    spec:
      storageClassName: local-path
      accessModes:
       - ReadWriteOnce
      resources:
        requests:
          storage: 3Gi
---
apiVersion: v1
kind: Endpoints
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
subsets: []

---
apiVersion: v1
kind: Service
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
spec:
  type: ClusterIP
  ports:
  - name: postgresql
    port: 5432
    targetPort: 5432

---
# headless service to avoid deletion of patronidemo-config endpoint
apiVersion: v1
kind: Service
metadata:
  name: zalandopatroni777-config
  labels:
    application: spilo
    spilo-cluster: zalandopatroni777
spec:
  clusterIP: None

---
apiVersion: v1
kind: Secret
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
type: Opaque
stringData:
  superuser-password: pass1
  replication-password: pass2
  admin-password: pass3

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: operator

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: operator
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
  # delete is required only for 'patronictl remove'
  - delete
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - get
  - patch
  - update
  # the following three privileges are necessary only when using endpoints
  - create
  - list
  - watch
  # delete is required only for for 'patronictl remove'
  - delete
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - patch
  - update
  - watch
# The following privilege is only necessary for creation of headless service
# for patronidemo-config endpoint, in order to prevent cleaning it up by the
# k8s master. You can avoid giving this privilege by explicitly creating the
# service like it is done in this manifest (lines 160..169)
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - create

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: operator
subjects:
- kind: ServiceAccount
  name: operator
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-script
data:
  PGHOST: "/var/run/postgresql"
  PGUSER: "postgres"
  PGROOT: "/home/postgres/pgdata/pgroot"
  PGLOG: "/home/postgres/pgdata/pgroot/pg_log"
  PGDATA: "/home/postgres/pgdata/pgroot/data"
  BACKUP_NUM_TO_RETAIN: "5"
  USE_WALG_BACKUP: "true"
  USE_WALG_RESTORE: "true"
  WAL_BUCKET_SCOPE_PREFIX: ""
  WAL_BUCKET_SCOPE_SUFFIX: ""
  WALG_ALIVE_CHECK_INTERVAL: "5m"
  WALE_BINARY: "wal-g"
  WALG_FILE_PREFIX: "/data/pg_wal"
  CLONE_USE_WALG_RESTORE: "true"
  WALG_DISABLE_S3_SSE: "true"
  WALE_ENV_DIR: "/config"




0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------







