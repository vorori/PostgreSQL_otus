0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------
                                       ИГРА 
									   Со мною все нормально
                                       Ну и что, что кровь из носа
                                       Со мною все нормально
                                       Просто я стал очень взрослым
0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------
-------------------------------------для тестов из вне------------------------------------------------
------------------------------------------------------------------------------------------------------

#проверяю что все ок и что все запустилось все под создались и работают
kubectl get pods --namespace spilo
kubectl get pods --namespace spilo

#проверяю что создались сервисы
kubectl get services --namespace spilo
kubectl get services --namespace spilo

#проверяю что все ок и что все запустилось с дисками pvc
kubectl get pvc --namespace spilo
kubectl get pvc --namespace spilo

#проверяю что все ок на всех обектах кластера
kubectl get all -A
kubectl get all -A

#подключиться к поду внутрь контенера с patroni
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- bash
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- bash

#смотрим на кворум кто мастер 
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl list
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl list

#единоразово смотрим логи pod видм кто у нас матер
kubectl logs --namespace spilo pod/zalandopatroni777-0
kubectl logs --namespace spilo pod/zalandopatroni777-1

#смотреть логи в режиме реального времени  
#namespace spilo используя префикс -f
kubectl logs --namespace spilo pod/zalandopatroni777-0 -f
kubectl logs --namespace spilo pod/zalandopatroni777-0 -f

#единоразово смотрим логи pod видм кто у нас матер
kubectl logs --namespace spilo pod/zalandopatroni777-0 --tail=40 
kubectl logs --namespace spilo pod/zalandopatroni777-0 --tail=40 

#Наборы скриптов для patroni находятся тут
kubectl -n spilo exec -it pod/zalandopatroni777-0 -- ls /scripts/
kubectl -n spilo exec -it pod/zalandopatroni777-0 -- ls /scripts/

kubectl -n spilo exec -it pod/zalandopatroni777-0 -- cat /scripts/basebackup.sh
kubectl -n spilo exec -it pod/zalandopatroni777-0 -- cat /scripts/basebackup.sh

#запустить скрипт бекапа
kubectl -n spilo exec -it pod/zalandopatroni777-0 -- bash /scripts/basebackup.sh
kubectl -n spilo exec -it pod/zalandopatroni777-0 -- bash /scripts/basebackup.sh

#если требуется restart конфигурацию кластера 
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl restart zalandopatroni777
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl restart zalandopatroni777

#если требуется reload конфигурацию кластера 
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl reload zalandopatroni777
kubectl -n spilo exec -it pod/zalandopatroni777-0  -- patronictl reload zalandopatroni777

#выполняю switchover
kubectl -n spilo exec -it pod/zalandopatroni777-1  -- patronictl switchover
kubectl -n spilo exec -it pod/zalandopatroni777-1  -- patronictl switchover

#удаляем поды 
kubectl delete pods zalandopatroni777-0 zalandopatroni777-1 zalandopatroni777-2 --namespace spilo
kubectl delete pods zalandopatroni777-0 zalandopatroni777-1 zalandopatroni777-2 --namespace spilo

#удаляем кластер из etcd
kubectl -n spilo exec -it pod/zalandopatroni777-1 -- patronictl remove zalandopatroni777
kubectl -n spilo exec -it pod/zalandopatroni777-1 -- patronictl remove zalandopatroni777


#установил на управляюшей ноде postgressudo но кластер не инициализировал мне нужна програмка pg_basebackup 
yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm && yum -y repolist && yum -y install postgresql15-contrib

#выполняю бекап postgres кластера который в кубере 
mkdir /var/postgre_backup
chown postgres:postgres /var/postgre_backup

#выполняю бекап pg_basebackup
su - postgres -c "/usr/pgsql-15/bin/pg_basebackup -U postgres -p 5432 -h 10.99.158.120 --progress --verbose --format=tar --gzip --pgdata=/var/postgre_backup/all_db_postgre_backup_`date +"%Y_%m_%d_%H:%M"`"
su - postgres -c "/usr/pgsql-15/bin/pg_basebackup -U postgres -p 5432 -h 10.99.158.120 --progress --verbose --format=tar --gzip --pgdata=/var/postgre_backup/all_db_postgre_backup_`date +"%Y_%m_%d_%H:%M"`"

#выполним backup wal-g
envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data
envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data

# список бекапов. смотрим какие бекапы у нас есть в наличии
envdir /config /usr/local/bin/wal-g backup-list
envdir /config /usr/local/bin/wal-g backup-list

### восстанавливаюсь на определенный бекап
/usr/local/bin/wal-g/wal-g-pg backup-fetch /var/lib/pgsql/15/data backup_name
/usr/local/bin/wal-g/wal-g-pg backup-fetch /var/lib/pgsql/15/data backup_name

#получить последний бекап из wal-g
envdir /config /usr/local/bin/wal-g backup-fetch /home/postgres/pgdata/pgroot/data LATEST
envdir /config /usr/local/bin/wal-g backup-fetch /home/postgres/pgdata/pgroot/data LATEST

------------------------------------------------------------------------------------------------------
-------------------------------------для тестов внутри контенера--------------------------------------
------------------------------------------------------------------------------------------------------
# для patroni
sv stop cron
sv restart patroni

#показать лидера и кворум 
patronictl -c postgres.yml list
patronictl -c postgres.yml list

#команда на изменение config
patronictl -c postgres.yml edit-config
patronictl -c postgres.yml edit-config

--------------------------------------
loop_wait: 10
master_start_timeout: 300
maximum_lag_on_failover: 1048576
postgresql:
  parameters:
    archive_mode: 'on'
    archive_timeout: 1800s
    autovacuum: true
	wal_buffers: 16MB
    wal_compression: true
    wal_keep_size: 2GB
    wal_level: replica
    wal_log_hints: true
    wal_receiver_status_interval: 10s
    wal_writer_delay: 200ms
    wal_writer_flush_after: 1MB
    work_mem: 32MB
  pg_hba:
  - host replication postgres 10.244.0.0/0 md5
  - host replication all 10.244.0.0/0 md5
  use_pg_rewind: true
  use_slots: true
retry_timeout: 10
synchronous_mode_strict: false
ttl: 100
--------------------------------------

#если требуется restart
patronictl -c postgres.yml restart zalandopatroni777
patronictl -c postgres.yml restart zalandopatroni777

#если требуется reload
patronictl -c postgres.yml reload zalandopatroni777
patronictl -c postgres.yml reload zalandopatroni777

#преключить wal
select pg_switch_wal();
select pg_switch_wal();

#БАЗОВАЯ ИНФОРМАЦИЯ порт версия сервер
select
inet_server_addr( ) AS "Server",
inet_server_port( ) AS "Port",
current_database() AS "CurrentDatabase",
version() AS "Version";

#показать ноды реплики
select usename,application_name,client_addr,backend_start,state,sync_state from pg_stat_replication;
select usename,application_name,client_addr,backend_start,state,sync_state from pg_stat_replication;

#показать состояние конкретной ноды к которой подключился не находится ли она в состоянии восстоновления
select pg_is_in_recovery();
select pg_is_in_recovery();







<pre>
============================================================================================
============================================================================================
============================================================================================
===========================установить настроить local-path-provisioner START================
============================================================================================
============================================================================================
============================================================================================
</pre>



1)
https://github.com/BigKAA/youtube/tree/38f295485674147bc484c2183625059987a46013/base/local-path-provisioner

установить настроить local-path-provisioner

скачать манифест редактируем по своим настройкам:
https://github.com/BigKAA/youtube/blob/38f295485674147bc484c2183625059987a46013/base/local-path-provisioner/manifests/00-local-path-storage.yaml

mkdir /data
mkdir /data/local-path-provisioner
vim /data/00-local-path-storage.yaml


В конфигурации по умолчанию, на всех нодах кластера, если потребуется разместить volume. 
Директория этого тома будет создаваться в /data/local-path-provisioner. Для каждого volume отдельная директория.

--------------------------------------------------------------------------------------
apiVersion: v1
kind: Namespace
metadata:
  name: local-path-storage

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: local-path-provisioner-service-account
  namespace: local-path-storage

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: local-path-provisioner-role
rules:
  - apiGroups: [ "" ]
    resources: [ "nodes", "persistentvolumeclaims", "configmaps" ]
    verbs: [ "get", "list", "watch" ]
  - apiGroups: [ "" ]
    resources: [ "endpoints", "persistentvolumes", "pods" ]
    verbs: [ "*" ]
  - apiGroups: [ "" ]
    resources: [ "events" ]
    verbs: [ "create", "patch" ]
  - apiGroups: [ "storage.k8s.io" ]
    resources: [ "storageclasses" ]
    verbs: [ "get", "list", "watch" ]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-path-provisioner-bind
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: local-path-provisioner-role
subjects:
  - kind: ServiceAccount
    name: local-path-provisioner-service-account
    namespace: local-path-storage

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: local-path-provisioner
  namespace: local-path-storage
spec:
  replicas: 1
  selector:
    matchLabels:
      app: local-path-provisioner
  template:
    metadata:
      labels:
        app: local-path-provisioner
    spec:
      serviceAccountName: local-path-provisioner-service-account
      containers:
        - name: local-path-provisioner
          image: rancher/local-path-provisioner:v0.0.24
          imagePullPolicy: IfNotPresent
          command:
            - local-path-provisioner
            - --debug
            - start
            - --config
            - /etc/config/config.json
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config/
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
      volumes:
        - name: config-volume
          configMap:
            name: local-path-config

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: local-path-config
  namespace: local-path-storage
data:
  config.json: |-
    {
            "nodePathMap":[
            {
                    "node":"DEFAULT_PATH_FOR_NON_LISTED_NODES",
                    "paths":["/data/local-path-provisioner"]
            }
            ]
    }
  setup: |-
    #!/bin/sh
    set -eu
    mkdir -m 0777 -p "$VOL_DIR"
  teardown: |-
    #!/bin/sh
    set -eu
    rm -rf "$VOL_DIR"
  helperPod.yaml: |-
    apiVersion: v1
    kind: Pod
    metadata:
      name: helper-pod
    spec:
      containers:
      - name: helper-pod
        image: busybox:1.35.0
        imagePullPolicy: IfNotPresent
--------------------------------------------------------------------------------------------------

заметка:
В local-path-provisioner:v0.0.24 ограничение по объему PV не поддерживается!

Установка local-path-provisioner

### посмотрим дефолтный тип стораджа
kubectl get storageclasses
kubectl get storageclasses
kubectl get storageclasses

#если надо удалить
kubectl delete storageclasses local-path
kubectl delete storageclasses local-path
kubectl delete storageclasses local-path

#применяем
kubectl apply -f /data/00-local-path-storage.yaml

#проверяем
kubectl get all -A
------------------------------
NAMESPACE            NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
kube-system          deployment.apps/coredns                  2/2     2            2           3d4h
local-path-storage   deployment.apps/local-path-provisioner   1/1     1            1           2m33s
-----------------------------

<pre>
============================================================================================
============================================================================================
============================================================================================
===========================установить настроить local-path-provisioner END==================
============================================================================================
============================================================================================
============================================================================================
</pre>





<pre>
============================================================================================
============================================================================================
============================================================================================
===========================Создаем PVC где его имя name: volume-test-pvc START==============
============================================================================================
============================================================================================
============================================================================================
</pre>

В файле my_pvc.yaml показан пример PVC.

vim /data/my_pvc.yaml 
vim /data/my_pvc.yaml 

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: volume-test-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 200Mi

Обычно в качестве локальных файловых систем нод кластера используются не кластерные файловые системы. Поэтому в PVC указываем accessModes ReadWriteOnce.
Добавляем PVC в кластер.

kubectl apply -f /data/my_pvc.yaml
kubectl apply -f /data/my_pvc.yaml

Проверяем состояние PVC.
------------------
[root@masterkub data]# kubectl get pvc
NAME              STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
volume-test-pvc   Pending                                      local-path     113s
------------------
status pending говорит нам о том что он не распределяет диск физически до тех пор пока к этому pvc никто не обратится

Т.е. в дальнейшем наше приложение можно будет запускать только на ноде, где создан PV для используемого в приложении PVC.


#черновик он нам понадобится чтобы понимание было как оно работает:
---
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: default
spec:
  containers:
  - name: volume-test
    image: nginx:1.23.4-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  nodeSelector:
    kubernetes.io/hostname: ws1.kryukov.local     #вот тут мы указываем где на какой ноде будет создан этот диск
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: volume-test-pvc




#Использование в StatefulSet
Как мы поняли из предыдущего примера, при использовании StorageClass связанных с local-path-provisioner мы должны внимательно следить где будут запускаться приложения.

Самым правильным вариантом, когда нам требуется сохранять состояния приложений в файловой системе, является использование StatefulSet.

При определении StatefulSet мы должны будем учесть следующие особенности:
1)Явным образом определить ноды, на которых будут запускаться pods StatefulSet-та.
2)Позаботиться о том, что бы на одной ноде запускался один pod StatefulSet-та.



<pre>
============================================================================================
============================================================================================
============================================================================================
===========================Создаем PVC где его имя name: volume-test-pvc END================
============================================================================================
============================================================================================
============================================================================================
</pre>




============================================================================================
============================================================================================
===========================#создаем окружения для  скрипта backup START=====================
============================================================================================
============================================================================================
============================================================================================

--------------------------
rm -rf /data/spilo_backup-script_suka.yaml
vim /data/spilo_backup-script_suka.yaml
cat /data/spilo_backup-script_suka.yaml
--------------------------

#применяю
--------------------------
kubectl apply -f /data/spilo_backup-script_suka.yaml
kubectl apply -f /data/spilo_backup-script_suka.yaml
--------------------------

#проверяем
--------------------------
kubectl get ConfigMap
kubectl get ConfigMap
--------------------------

#если надо удалить
--------------------------
kubectl delete configmap backup-script
kubectl delete configmap backup-script
--------------------------

--------------------------
--------------------------
--------------------------
--------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-script
data:
  PGHOST: "/var/run/postgresql"
  PGUSER: "postgres"
  PGROOT: "/home/postgres/pgdata/pgroot"
  PGLOG: "/home/postgres/pgdata/pgroot/pg_log"
  PGDATA: "/home/postgres/pgdata/pgroot/data"
  BACKUP_NUM_TO_RETAIN: "5"
  USE_WALG_BACKUP: "true"
  USE_WALG_RESTORE: "true"
  WAL_BUCKET_SCOPE_PREFIX: ""
  WAL_BUCKET_SCOPE_SUFFIX: ""
  WALG_ALIVE_CHECK_INTERVAL: "5m"
  WALE_BINARY: "wal-g"
  WALG_FILE_PREFIX: "/data/pg_wal"
  CLONE_USE_WALG_RESTORE: "true"
  WALG_DISABLE_S3_SSE: "true"
  WALE_ENV_DIR: "/config
--------------------------
--------------------------
--------------------------
--------------------------


============================================================================================
============================================================================================
============================================================================================
===========================#создаем окружения для  скрипта backup END=======================
============================================================================================
============================================================================================
============================================================================================


============================================================================================
============================================================================================
============================================================================================
===========================##начинаем создание нашего кластера START========================
============================================================================================
============================================================================================
============================================================================================


#создаем окружения для  основной конфигурации
-------------------------- 
rm -rf /data/spilo_kubernetes_final_suka.yaml
vim /data/spilo_kubernetes_final_suka.yaml
cat /data/spilo_kubernetes_final_suka.yaml
--------------------------


#создаем namespase spilo
--------------------------
kubectl create ns spilo
kubectl create ns spilo
--------------------------

#применяю
--------------------------
kubectl apply -f /data/spilo_kubernetes_final_suka.yaml --namespace spilo
kubectl apply -f /data/spilo_kubernetes_final_suka.yaml --namespace spilo
--------------------------

#проверяю
--------------------------
# pods
kubectl get pods --namespace spilo
kubectl get pods --namespace spilo

# pwc
kubectl get pvc --namespace spilo
kubectl get pvc --namespace spilo

# Это предоставит информацию о пространствах имен
kubectl get namespace
kubectl get namespace
--------------------------


#если надо удалить
--------------------------
#если надо удалить манифест
kubectl delete -f /data/spilo_kubernetes_final_suka.yaml  --namespace spilo
kubectl delete -f /data/spilo_kubernetes_final_suka.yaml  --namespace spilo

#если надо удалить  all,ing,secrets,pvc,pv
kubectl delete all,ing,secrets,pvc,pv --all --namespace spilo
kubectl delete all,ing,secrets,pvc,pv --all --namespace spilo

#после удаляю руками persistentvolume которые не удалились автоматом  из директории  
rm -rf /data/local-path-provisioner/*
rm -rf /data/local-path-provisioner/*
rm -rf /data/local-path-provisioner/*

# дополнительно если надо удалить  namespace spilo
kubectl delete namespace spilo
kubectl delete namespace spilo
--------------------------


--------------------------
--------------------------
--------------------------
--------------------------
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
spec:
  selector:
    matchLabels:
      application: spilo
      spilo-cluster: *cluster_name
  replicas: 3
  serviceName: *cluster_name
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        application: spilo
        spilo-cluster: *cluster_name
    spec:
      # service account that allows changing endpoints and assigning pod labels
      # in the given namespace: https://kubernetes.io/docs/user-guide/service-accounts/
      # not required unless you've changed the default service account in the namespace
      # used to deploy Spilo
      serviceAccountName: operator
      containers:
      - name: *cluster_name
        image: registry.opensource.zalan.do/acid/spilo-15:3.0-p1  # put the spilo image here
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8008
          protocol: TCP
        - containerPort: 5432
          protocol: TCP
        volumeMounts:
        - mountPath: /data/pg_wal
          name: backup
        - mountPath: /config
          name: config
        - mountPath: /home/postgres/pgdata
          name: pgdata
        env:
        - name: DCS_ENABLE_KUBERNETES_API
          value: 'true'
#        - name: ETCD_HOST
#          value: 'test-etcd.default.svc.cluster.local:2379' # where is your etcd?
#        - name: WAL_S3_BUCKET
#          value: example-spilo-dbaas
#        - name: LOG_S3_BUCKET # may be the same as WAL_S3_BUCKET
#          value: example-spilo-dbaas
#        - name: BACKUP_SCHEDULE
#          value: "00 01 * * *"
        - name: KUBERNETES_SCOPE_LABEL
          value: spilo-cluster
        - name: KUBERNETES_ROLE_LABEL
          value: role
        - name: SPILO_CONFIGURATION
          value: | ## https://github.com/zalando/patroni#yaml-configuration
            bootstrap:
              dcs:
                ttl: 100
                loop_wait: 10
                retry_timeout: 10
                maximum_lag_on_failover: 1048576
                master_start_timeout: 300
                synchronous_mode_strict: false
                postgresql:
                  use_pg_rewind: true
                  use_slots: true
                  parameters:
                    max_connections: 101
                    shared_buffers : 2GB
                    effective_cache_size : 7GB
                    maintenance_work_mem: 512MB
                    wal_buffers: 16MB
                    wal_keep_size: 2GB
                    work_mem: 32MB
                    min_wal_size: 1GB
                    max_wal_size: 4GB
                    huge_pages: off
                    max_worker_processes: 4
                    max_parallel_workers: 4
                    max_parallel_workers_per_gather: 2
                    max_parallel_maintenance_workers: 2
                    autovacuum: on
                    autovacuum_max_workers: 4
                    autovacuum_vacuum_scale_factor: 0.01
                    autovacuum_analyze_scale_factor: 0.03
                    autovacuum_vacuum_cost_limit: 500
                    autovacuum_vacuum_cost_delay: 2
                    autovacuum_naptime: 15s
                    autovacuum_vacuum_threshold: 20
                    wal_writer_delay : 200ms
                    wal_writer_flush_after : 1MB
                    random_page_cost: 1.1
                    seq_page_cost: 1
                    effective_io_concurrency: 200
                    superuser_reserved_connections: 4
                    max_locks_per_transaction: 64
                    max_prepared_transactions: 0
                    checkpoint_timeout: 10min
                    checkpoint_completion_target: 0.9
                    default_statistics_target: 1000
                    synchronous_commit: on
                    max_files_per_process: 1024
                    wal_level: replica
                    max_wal_senders: 10
                    max_replication_slots: 10
                    hot_standby: on
                    wal_compression: on
                    track_io_timing: on
                    log_lock_waits: on
                    log_temp_files: 0
                    track_activities: on
                    track_counts: on
                    track_functions: all
                    log_checkpoints: off
                    log_connections: off
                    log_disconnections: off
                    log_statement: none
                    logging_collector: on
                    log_min_duration_statement: 30s
                    log_truncate_on_rotation: on
                    log_rotation_age: 1d
                    log_rotation_size: 0
                    log_line_prefix: '%m [%p] %d %u %h (transaction ID %x)'
                    max_standby_streaming_delay: 30s
                    wal_receiver_status_interval: 10s
                    jit: off
                    lc_messages: en_US.UTF-8
                    track_commit_timestamp: "off"
                    wal_log_hints: on
                    hot_standby_feedback: off
                  pg_hba:
                  - hostnossl all     vororinew      all          md5
              initdb:
                - encoding: UTF8
                - data-checksums
              pg_hba:
              - hostnossl all         vorori         all          md5
            postgresql:
                parameters:
                  log_destination: 'stderr'
                  log_line_prefix: '%m [%p] %d %u %h (transaction ID %x)'
                  archive_mode: on
                  archive_command: 'envdir /config /usr/local/bin/wal-g wal-push "%p"'
                recovery_conf:
                  restore_command: 'envdir /config /usr/local/bin/wal-g wal-fetch "%f" "%p"'
                pg_hba:
                - local     all         all                    trust
                - hostssl   all         +zalandos 127.0.0.1/32 pam
                - host      all         all       127.0.0.1/32 md5
                - hostssl   all         +zalandos ::1/128      pam
                - host      all         all       ::1/128      md5
                - local     replication standby                trust
                - hostssl   replication standby   all          md5
                - hostnossl all         all       all          md5
                - hostssl   all         +zalandos all          pam
                - hostssl   all         all       all          md5
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: PGPASSWORD_SUPERUSER
          valueFrom:
            secretKeyRef:
              name: *cluster_name
              key: superuser-password
        - name: PGUSER_ADMIN
          value: superadmin
        - name: PGPASSWORD_ADMIN
          valueFrom:
            secretKeyRef:
              name: *cluster_name
              key: admin-password
        - name: PGPASSWORD_STANDBY
          valueFrom:
            secretKeyRef:
              name: *cluster_name
              key: replication-password
        - name: SCOPE
          value: *cluster_name
        - name: PGROOT
          value: /home/postgres/pgdata/pgroot
        - name: WALG_FILE_PREFIX
          value: "/home/postgres/pgdata/pgroot/pg_log"
        - name: CRONTAB
          value: "[\"00 01 * * * envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data\"]"
      terminationGracePeriodSeconds: 0
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: db
                    operator: In
                    values:
                      - spilo
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: spilo-cluster
                    operator: In
                    values:
                      - *cluster_name
              topologyKey: "kubernetes.io/hostname"
      volumes:
        - configMap:
            name: backup-script
          name: config
        - persistentVolumeClaim:
            claimName: zalandopatroni777-backup
          name: backup
  volumeClaimTemplates:
  - metadata:
      labels:
        application: spilo
        spilo-cluster: *cluster_name
      name: pgdata
    spec:
      storageClassName: local-path
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
  - metadata:
      labels:
        application: spilo
        spilo-cluster: *cluster_name
      name: backup
    spec:
      storageClassName: local-path
      accessModes:
       - ReadWriteOnce
      resources:
        requests:
          storage: 3Gi
---
apiVersion: v1
kind: Endpoints
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
subsets: []

---
apiVersion: v1
kind: Service
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
spec:
  type: ClusterIP
  ports:
  - name: postgresql
    port: 5432
    targetPort: 5432

---
# headless service to avoid deletion of patronidemo-config endpoint
apiVersion: v1
kind: Service
metadata:
  name: zalandopatroni777-config
  labels:
    application: spilo
    spilo-cluster: zalandopatroni777
spec:
  clusterIP: None

---
apiVersion: v1
kind: Secret
metadata:
  name: &cluster_name zalandopatroni777
  labels:
    application: spilo
    spilo-cluster: *cluster_name
type: Opaque
stringData:
  superuser-password: pass1
  replication-password: pass2
  admin-password: pass3

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: operator

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: operator
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
  # delete is required only for 'patronictl remove'
  - delete
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - get
  - patch
  - update
  # the following three privileges are necessary only when using endpoints
  - create
  - list
  - watch
  # delete is required only for for 'patronictl remove'
  - delete
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - patch
  - update
  - watch
# The following privilege is only necessary for creation of headless service
# for patronidemo-config endpoint, in order to prevent cleaning it up by the
# k8s master. You can avoid giving this privilege by explicitly creating the
# service like it is done in this manifest (lines 160..169)
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - create

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: operator
subjects:
- kind: ServiceAccount
  name: operator
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-script
data:
  PGHOST: "/var/run/postgresql"
  PGUSER: "postgres"
  PGROOT: "/home/postgres/pgdata/pgroot"
  PGLOG: "/home/postgres/pgdata/pgroot/pg_log"
  PGDATA: "/home/postgres/pgdata/pgroot/data"
  BACKUP_NUM_TO_RETAIN: "5"
  USE_WALG_BACKUP: "true"
  USE_WALG_RESTORE: "true"
  WAL_BUCKET_SCOPE_PREFIX: ""
  WAL_BUCKET_SCOPE_SUFFIX: ""
  WALG_ALIVE_CHECK_INTERVAL: "5m"
  WALE_BINARY: "wal-g"
  WALG_FILE_PREFIX: "/data/pg_wal"
  CLONE_USE_WALG_RESTORE: "true"
  WALG_DISABLE_S3_SSE: "true"
  WALE_ENV_DIR: "/config"
--------------------------
--------------------------
--------------------------
--------------------------


============================================================================================
============================================================================================
============================================================================================
===========================##начинаем создание нашего кластера END==========================
============================================================================================
============================================================================================
============================================================================================


============================================================================================
============================================================================================
============================================================================================
====================================с чем столкнулся START==================================
============================================================================================
============================================================================================
============================================================================================
# пытаюсь починить archive_command

https://github.com/zalando/postgres-operator/issues/1233
https://github.com/zalando/postgres-operator/issues/1233
============================================================================================

КОМАНДА КОТОРАЯ СРАБОТАЛА!!!!!!!!!! ВРУЧНУЮ создать бекап wal файла 000000020000000000000007!!!!!!!!!!!!!
wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"

/usr/local/bin/wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
/usr/local/bin/wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"
/usr/local/bin/wal-g wal-push "/home/postgres/pgdata/pgroot/data/pg_wal/000000020000000000000007"

envdir /config /usr/local/bin/wal-g wal-push /home/postgres/pgdata/pgroot/data/pg_wal/000000010000000000000009
============================================================================================
============================================================================================
============================================================================================






============================================================================================
============================================================================================
============================================================================================
# восстановление кластера из бекапа wal-g с журналами на точку времени
https://github.com/zalando/patroni/issues/1571
https://github.com/zalando/patroni/issues/1571
============================================================================================


# моя заметка 
# WAL-G 4 команды. Это:
WAL-PUSH – заархивировать вал.

WAL-FETCH – получить вал.

BACKUP-PUSH – сделать бэкап.

BACKUP-FETCH – получить бэкап из системы резервного копирования.


----------------------------------------------------------------------------------------------------------
Спасибо за помощь! Теперь он работает, так что вот мой файл конфигурации и скрипты. 
Это рабочая комбинация для Postgresql 12 + Patroni + WAL-G (поддерживаемый преемник WAL-E) + Azure/AW
----------------------------------------------------------------------------------------------------------

#как восстанавливаться все расписсанно по щагам
----------------------------------------------------------------------------------------------------------
Предупреждение: ваш каталог /data должен быть доступен для записи пользователю postgres.

Шаги к PITR:

1)
Остановите Patroni на всех узлах реплик и, наконец, на мастере
sudo systemctl stop Patroni

2)
Обновить файл конфигурации /etc/patroni.yml
recovery_target_time: '2020-06-08 08:52:00'

3)
Удалить кластер из etcd
patronictl -c /etc/patroni.yml remove postgres

4)
Резервное копирование и удаление каталога данных на мастере /data/patroni

5)
Запустите Patroni на мастере — он автоматически вызовет скрипт clone_with_walg.sh
sudo systemctl start patchi
----------------------------------------------------------------------------------------------------------


#пример который работает с восстановлением
----------------------------------------------------------------------------------------------------------
scope: postgres
namespace: /db/
name: postgresql1

restapi:
    listen: 10.0.1.4:8008
    connect_address: 10.0.1.4:8008

etcd:
    host: 10.0.1.7:2379

bootstrap:
    dcs:
        ttl: 30
        loop_wait: 10
        retry_timeout: 10
        maximum_lag_on_failover: 1048576
        postgresql:
            use_pg_rewind: true

    method: clone_with_walg
    clone_with_walg:
        command: /home/postgres/clone_with_walg.sh
        recovery_conf:
            restore_command: envdir /etc/wal-g.d/env wal-g wal-fetch "%f" "%p"
            recovery_target_timeline: latest
            recovery_target_action: promote
            recovery_target_time: ''

    initdb:
    - encoding: UTF8
    - data-checksums

    pg_hba:
    - host replication replicator 127.0.0.1/32 md5
    - host replication replicator 10.0.1.4/0 md5
    - host replication replicator 10.0.1.8/0 md5
    - host replication replicator 10.0.1.6/0 md5
    - host all all 0.0.0.0/0 md5

postgresql:
    listen: 10.0.1.4:5432
    connect_address: 10.0.1.4:5432
    data_dir: /data/patroni
    pgpass: /tmp/pgpass
    authentication:
        replication:
            username: replicator
            password: rep-pass
        superuser:
            username: postgres
            password: secretpassword
    parameters:
        unix_socket_directories: '/var/run/postgresql'
        shared_preload_libraries: 'pg_stat_statements'
        archive_mode: 'on'
        archive_timeout: 300s
        archive_command: 'envdir /etc/wal-g.d/env wal-g wal-push %p'
    recovery_conf:
        restore_command: 'envdir /etc/wal-g.d/env wal-g wal-fetch "%f" "%p"'

tags:
    nofailover: false
    noloadbalance: false
    clonefrom: false
    nosync: false
----------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------
И простой скрипт clone_with_walg.sh:

#!/bin/bash

mkdir -p /data/patroni
envdir /etc/wal-g.d/env wal-g backup-fetch /data/patroni LATEST
----------------------------------------------------------------------------------------------------------





----------------------------------------------------------------------------------------------------------
#ЧЕРНОВИК
https://github.com/zalando/postgres-operator/blob/master/docs/administrator.md
https://github.com/zalando/patroni/issues/1571
https://blog.csdn.net/chenhongloves/article/details/123236332
https://blog.csdn.net/chenhongloves/article/details/123236332
https://blog.csdn.net/chenhongloves/article/details/123236332

В зависимости от поставщика облачного хранилища для Spilo должны быть установлены разные переменные среды . 
Не все они генерируются автоматически оператором путем изменения его конфигурации. 
В этом случае вы должны использовать дополнительную карту конфигурации или секрет .

#выполним backup
envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data
envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data
envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data

В Postgres вы можете проверить предварительно настроенные команды для архивации и восстановления файлов WAL. 
Вы можете найти файлы журналов для соответствующих команд в разделе $HOME/pgdata/pgroot/pg_log/postgres-?.log.

archive_mode: 'on'
archive_command:  `envdir "{WALE_ENV_DIR}" {WALE_BINARY} wal-push "%p"`
restore_command:  `envdir "{{WALE_ENV_DIR}}" /scripts/restore_command.sh "%f" "%p"`


Вы можете создать базовую резервную копию вручную с помощью следующей команды и проверить, попадает ли она в указанный вами путь резервного копирования WAL:
envdir "/run/etc/wal-e.d/env" /scripts/postgres_backup.sh "/home/postgres/pgdata/pgroot/data"

Вы также можете проверить, может ли Spilo найти какие-либо резервные копии:
envdir "/run/etc/wal-e.d/env" wal-g backup-list
----------------------------------------------------------------------------------------------------------

============================================================================================
============================================================================================
============================================================================================
====================================с чем столкнулся END====================================
============================================================================================
============================================================================================
============================================================================================




0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------

0--------------------------------------------------------------------------------------------------------







